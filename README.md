#ğŸŒ¸ Iris Veri Seti SÄ±nÄ±flandÄ±rma Analizi

Bu proje, makine Ã¶ÄŸrenmesine giriÅŸin "Merhaba DÃ¼nya"sÄ± kabul edilen Iris Veri Seti Ã¼zerinde, teorik olarak Ã¶ÄŸrendiÄŸim sÄ±nÄ±flandÄ±rma (classification) algoritmalarÄ±nÄ±n pratik performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmak amacÄ±yla geliÅŸtirdiÄŸim bir Ã§alÄ±ÅŸmadÄ±r.


##ğŸ“‚ Veri Seti HakkÄ±nda (Dataset)
Analizimde kullandÄ±ÄŸÄ±m veri seti 150 satÄ±r ve 5 sÃ¼tundan oluÅŸmaktadÄ±r.
**Girdi Ã–zellikleri (Features):**
-> sepal length: Ã‡anak yaprak uzunluÄŸu
-> sepal width: Ã‡anak yaprak geniÅŸliÄŸi
-> petal length: TaÃ§ yaprak uzunluÄŸu
-> petal width: TaÃ§ yaprak geniÅŸliÄŸi
**Hedef SÄ±nÄ±flar (Target):**
-> Iris-setosa
-> Iris-versicolor
-> Iris-virginica
Not: Veri setinde her sÄ±nÄ±ftan eÅŸit sayÄ±da (50'ÅŸer adet) Ã¶rnek bulunmaktadÄ±r, yani dengeli (balanced) bir veri setidir.

###âš™ï¸ Ã–n Ä°ÅŸleme (Preprocessing)
Modelleri eÄŸitmeden Ã¶nce veriyi daha anlamlÄ± hale getirmek iÃ§in ÅŸu adÄ±mlarÄ± uyguladÄ±m:
**Veri BÃ¶lme:** Veriyi %67 EÄŸitim, %33 Test olacak ÅŸekilde ayÄ±rdÄ±m (test_size=0.33).
**Ã–lÃ§eklendirme (Scaling):** AlgoritmalarÄ±n (Ã¶zellikle KNN ve SVM) mesafeye dayalÄ± hesaplamalarÄ±nda hata yapmamasÄ± iÃ§in StandardScaler kullanarak verileri standartlaÅŸtÄ±rdÄ±m.

##ğŸ§  KullandÄ±ÄŸÄ±m Algoritmalar ve Analizler

###1. Logistic Regression (Lojistik Regresyon)
**SonuÃ§ (Hata SayÄ±sÄ±):** 0 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 15  0]
 [ 0  0 16]]
**Analiz: **Iris veri setinde sÄ±nÄ±flar doÄŸrusal olarak  Ã§ok net ayrÄ±labiliyor. Lojistik Regresyon, verideki bu doÄŸrusal sÄ±nÄ±rlarÄ± mÃ¼kemmel yakaladÄ± ve %100 baÅŸarÄ± saÄŸladÄ±. En verimli Ã§alÄ±ÅŸan modelim bu oldu.

###2. K-Nearest Neighbors (KNN)
**Parametre:** n_neighbors=1, metric='minkowski'
**SonuÃ§ (Hata SayÄ±sÄ±):** 3 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 14  1]
 [ 0  2 14]]
**Analiz: **En yakÄ±n tek komÅŸuya (k=1) bakarak karar vermek, modeli"gÃ¼rÃ¼ltÃ¼ye" karÅŸÄ± hassas hale getirdi. SÄ±nÄ±f sÄ±nÄ±rlarÄ±nda kalan 3 adet veriyi, sadece en yakÄ±nÄ±ndaki tek bir yanlÄ±ÅŸ komÅŸuya bakarak hatalÄ± sÄ±nÄ±flandÄ±rdÄ±. k deÄŸerini 3 veya 5 seÃ§mek Ã§ok daha iyi sonuÃ§ verecektir.

###3. Support Vector Machine (SVM)
**Parametre:** kernel='linear'
**SonuÃ§ (Hata SayÄ±sÄ±):** 1 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 14  1]
 [ 0  0 16]]
**Analiz**: SVM, sÄ±nÄ±flar arasÄ±nda en geniÅŸ gÃ¼venli yolu  Ã§izmeye Ã§alÄ±ÅŸÄ±r. Linear kernel kullanmama raÄŸmen, veri uzayÄ±nda sadece 1 noktada yanÄ±ldÄ±. OldukÃ§a kararlÄ±  bir model.

###4. Naive Bayes (Gaussian)
**SonuÃ§ (Hata SayÄ±sÄ±):** 2 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 14  1]
 [ 0  1 15]]
**Analiz**: Bu algoritma, "tÃ¼m Ã¶zellikler birbirinden baÄŸÄ±msÄ±zdÄ±r" (Ã¶rneÄŸin yaprak geniÅŸliÄŸi ile uzunluÄŸu alakasÄ±zdÄ±r) gibi saf bir varsayÄ±mla Ã§alÄ±ÅŸÄ±r. Biyolojik verilerde bu varsayÄ±m her zaman tutmasa da, algoritma ÅŸaÅŸÄ±rtÄ±cÄ± derecede hÄ±zlÄ± ve baÅŸarÄ±lÄ± Ã§alÄ±ÅŸarak sadece 2 hata yaptÄ±.

###5. Decision Tree (Karar AÄŸacÄ±)
**Parametre:** criterion='entropy'
**SonuÃ§ (Hata SayÄ±sÄ±):** 2 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 14  1]
 [ 0  1 15]]
**Analiz**: Veriyi "yaprak uzunluÄŸu < 2.4 ise Setosa'dÄ±r" gibi kurallara bÃ¶lerek aÄŸaÃ§ oluÅŸturdu. KÃ¼Ã§Ã¼k veri setlerinde aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski olsa da, entropy bu durum iÃ§in iyi bir seÃ§im oldu.

###6. Random Forest (Rastgele Orman)
**Parametre:** n_estimators=10, criterion='entropy'
**SonuÃ§ (Hata SayÄ±sÄ±):** 1 Hata
**Confusion Matrix:**
[[19  0  0]
 [ 0 15  0]
 [ 0  1 15]]
**Analiz**: Tek bir karar aÄŸacÄ± 2 hata yaparken, 10 aÄŸaÃ§tan oluÅŸan bu orman modeli hatayÄ± 1'e dÃ¼ÅŸÃ¼rdÃ¼. "Topluluk Ã–ÄŸrenmesi" sayesinde, tek bir aÄŸacÄ±n yaptÄ±ÄŸÄ± hatayÄ± Ã§oÄŸul aÄŸaÃ§alar kullanrak daha kararlÄ± hale getirdim.

**SonuÃ§ olarak;** hiÃ§bir algoritma evrensel olarak 'kusursuz' veya 'hatalÄ±' deÄŸildir. Ã–nemli olan, eldeki verinin doÄŸasÄ±nÄ± anlayarak ona en uygun algoritmayÄ± seÃ§mek ve parametreleri bu doÄŸrultuda optimize etmektir.

##ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma
**Projeyi kendi bilgisayarÄ±nÄ±zda Ã§alÄ±ÅŸtÄ±rmak iÃ§in:**
**Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin:**
** -> pip install numpy pandas scikit-learn xlrd openpyxl**
** -> Repoyu klonlayÄ±n ve kodu Ã§alÄ±ÅŸtÄ±rÄ±n.**


